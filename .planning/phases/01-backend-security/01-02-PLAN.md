---
phase: 01-backend-security
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - api/ask.ts
  - src/lib/mock-api.ts
  - src/lib/api-client.ts
autonomous: true

must_haves:
  truths:
    - "POST /api/ask accepts { question } and returns { lyric } on success"
    - "Rate-limited requests return 429 with 'Take a breath. Come back in a bit.'"
    - "Prompt injection attempts return 400 with warm error message"
    - "OpenAI timeout or failure returns 500 with 'Something went wrong, try again'"
    - "OpenAI API key never appears in client-side code or browser network tabs"
    - "Local development uses mock that returns same response shape without real API calls"
  artifacts:
    - path: "api/ask.ts"
      provides: "Vercel serverless function — the secure proxy"
      exports: ["POST"]
    - path: "src/lib/mock-api.ts"
      provides: "Local development mock API"
      exports: ["mockAskAPI"]
    - path: "src/lib/api-client.ts"
      provides: "Frontend API client that switches between mock and real"
      exports: ["askQuestion"]
  key_links:
    - from: "api/ask.ts"
      to: "src/lib/sanitize.ts"
      via: "sanitizeInput call before OpenAI"
      pattern: "sanitizeInput"
    - from: "api/ask.ts"
      to: "src/lib/ratelimit.ts"
      via: "rateLimiter.limit and dailyRateLimiter.limit calls"
      pattern: "rateLimiter\\.limit|dailyRateLimiter\\.limit"
    - from: "api/ask.ts"
      to: "src/lib/openai.ts"
      via: "openai.chat.completions.create call"
      pattern: "openai\\.chat\\.completions\\.create"
    - from: "src/lib/api-client.ts"
      to: "src/lib/mock-api.ts"
      via: "conditional import based on environment"
      pattern: "import\\.meta\\.env\\.DEV|NODE_ENV"
---

<objective>
Create the Vercel serverless function that composes the library modules from Plan 01 into a working API proxy, plus a local development mock and frontend API client.

Purpose: This is the core deliverable of Phase 1 — the secure proxy that protects the OpenAI API key from exposure, rate-limits requests, and sanitizes input. The mock API enables local development without Vercel or real API calls.

Output: Working `api/ask.ts` serverless function, `src/lib/mock-api.ts` for local development, `src/lib/api-client.ts` for frontend consumption.
</objective>

<execution_context>
@/Users/august/.claude/get-shit-done/workflows/execute-plan.md
@/Users/august/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-backend-security/01-RESEARCH.md
@.planning/phases/01-backend-security/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Vercel serverless API endpoint</name>
  <files>api/ask.ts</files>
  <action>
Create `api/ask.ts` — the single Vercel serverless function that handles all lyric requests.

**Export a `POST` function handler** (Vercel Web API pattern, NOT the legacy `(req, res)` pattern):

```typescript
export async function POST(request: Request): Promise<Response>
```

**Request flow (in this exact order):**

1. **Extract client IP** from `x-forwarded-for` header:
   - `request.headers.get('x-forwarded-for')` — take first IP if comma-separated
   - Fallback to `'unknown'` if header missing

2. **Check hourly rate limit** — call `rateLimiter.limit(clientIP)`:
   - If `!success`: return 429 with `{ error: "Take a breath. Come back in a bit." }` (exact locked message)
   - Include `retryAfter` in response body (seconds until reset, from `reset` property)

3. **Check daily rate limit** — call `dailyRateLimiter.limit(clientIP)`:
   - If `!success`: return 429 with `{ error: "Take a breath. Come back in a bit." }` (same message for both limits)

4. **Parse request body** — `await request.json()`:
   - Extract `question` field
   - If missing or not a string: return 400 with `{ error: "Something went wrong, try again" }`

5. **Sanitize input** — call `sanitizeInput(question)`:
   - If `!safe`: return 400 with `{ error: validation.error }` (the warm rejection message from sanitize.ts)

6. **Call OpenAI API** — use imported `openai` client:
   ```typescript
   const completion = await openai.chat.completions.create({
     model: "gpt-4o-mini",
     messages: [
       { role: "system", content: "You are helping match a life question to a Taylor Swift lyric. Return only the lyric text, nothing else." },
       { role: "user", content: question }
     ],
     max_tokens: 150,
   });
   ```
   - Use `gpt-4o-mini` for cost efficiency (this is a Phase 1 proxy — Phase 2 will refine the prompt and model choice)
   - Extract lyric from `completion.choices[0]?.message?.content`
   - If no content: return 500 with `{ error: "Something went wrong, try again" }`

7. **Return success** — 200 with `{ lyric }` (locked response shape)

**Error handling (catch block):**
- If `error.code === 'ETIMEDOUT'` or `error.name` contains 'Timeout': return 504 with `{ error: "Something went wrong, try again" }` (locked error message for timeout)
- All other errors: log `console.error('API error:', error)` and return 500 with `{ error: "Something went wrong, try again" }` (locked error message — honest, no fallback lyric)

**All responses** must include `{ 'Content-Type': 'application/json' }` header.

**Import paths** from api/ask.ts to src/lib/ use relative paths: `'../src/lib/openai'`, `'../src/lib/ratelimit'`, `'../src/lib/sanitize'`.

**IMPORTANT:** This function runs ONLY on Vercel, never in the browser. It has full access to process.env.
  </action>
  <verify>
- `api/ask.ts` exists and exports a `POST` function
- Function imports from `../src/lib/openai`, `../src/lib/ratelimit`, `../src/lib/sanitize`
- Rate limit check happens BEFORE OpenAI call (cost protection)
- Input sanitization happens BEFORE OpenAI call (injection protection)
- All error responses use warm, non-technical messages
- Rate limit message is exactly "Take a breath. Come back in a bit."
- Error message is exactly "Something went wrong, try again"
- Response shape is `{ lyric: string }` on success
- No API key or sensitive data in response payloads
  </verify>
  <done>Vercel serverless function at api/ask.ts accepts POST requests, extracts IP, checks dual rate limits, sanitizes input, calls OpenAI with 10-second timeout, and returns lyric or warm error message. Rate limiting and sanitization happen before any OpenAI call to protect credentials and prevent cost spikes.</done>
</task>

<task type="auto">
  <name>Task 2: Create mock API and frontend API client</name>
  <files>src/lib/mock-api.ts, src/lib/api-client.ts</files>
  <action>
**1. Create `src/lib/mock-api.ts`** — local development stub that mimics production response shapes without real API calls:

```typescript
export async function mockAskAPI(question: string): Promise<{ lyric?: string; error?: string }>
```

- Simulate network delay: `await new Promise(resolve => setTimeout(resolve, 800 + Math.random() * 400))` (800-1200ms — feels realistic)
- Return a hardcoded lyric for any valid input: `{ lyric: "Long story short, I survived" }` (a real Taylor Swift lyric that works as a generic test response)
- If question is empty string: return `{ error: "I couldn't understand that one. Try asking differently?" }` (matches production error shape)
- If question length > 200: return `{ error: "I couldn't understand that one. Try asking differently?" }` (matches production sanitization)
- No rate limiting simulation needed locally — keeps mock simple (Claude's discretion per CONTEXT.md)

**2. Create `src/lib/api-client.ts`** — the single function the React frontend will call:

```typescript
export async function askQuestion(question: string): Promise<{ lyric?: string; error?: string }>
```

- Import types from `../types/api` for type safety
- Use `import.meta.env.DEV` (Vite's built-in flag) to determine environment:
  - **Development (`import.meta.env.DEV === true`):** Import and call `mockAskAPI` from `./mock-api`
  - **Production (`import.meta.env.DEV === false`):** Call `fetch('/api/ask', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ question }) })`
- Parse production response and return `{ lyric }` or `{ error }`
- On fetch failure (network error): return `{ error: "Something went wrong, try again" }`

**Key design decisions:**
- Frontend imports ONLY `askQuestion` from `api-client.ts` — never directly calls fetch or mock
- Mock is a separate file (not inline) so it can be imported conditionally
- Production fetch uses relative URL `/api/ask` — Vercel routes this to the serverless function automatically when frontend and API are on the same domain
- The mock does NOT import sanitize.ts or ratelimit.ts — it's intentionally simple (per locked decision: "mock locally, no local serverless runtime needed")
  </action>
  <verify>
- `src/lib/mock-api.ts` exports `mockAskAPI` function
- `src/lib/api-client.ts` exports `askQuestion` function
- Mock returns `{ lyric: "Long story short, I survived" }` for valid inputs
- Mock returns `{ error: "..." }` for empty or over-200-char inputs
- API client uses `import.meta.env.DEV` to switch between mock and real
- Production path calls `fetch('/api/ask')` with POST method
- Both mock and real paths return same `{ lyric?, error? }` shape
- `npm run build` succeeds (type check + Vite bundle)
  </verify>
  <done>Mock API returns realistic response shapes with simulated delay for local development. Frontend API client provides single `askQuestion` function that automatically uses mock in development and real API in production. Frontend never directly accesses OpenAI credentials.</done>
</task>

</tasks>

<verification>
1. `api/ask.ts` exists with exported POST handler
2. Request flow order: IP extraction -> rate limit -> parse body -> sanitize -> OpenAI -> response
3. All three library modules (openai, ratelimit, sanitize) are imported and used in api/ask.ts
4. `src/lib/api-client.ts` provides the only frontend entry point for API calls
5. `npm run build` succeeds — no type errors, Vite bundles frontend correctly
6. No `process.env.OPENAI_API_KEY` appears in any file under `src/lib/api-client.ts` or `src/lib/mock-api.ts` (client-side files)
</verification>

<success_criteria>
- Serverless function composes rate limiting, sanitization, and OpenAI proxy into a single secure endpoint
- Local development works with mock — no Vercel runtime or API keys needed
- Frontend has a single, environment-aware API client function
- OpenAI API key is only accessed server-side in api/ask.ts via imported openai.ts module
</success_criteria>

<output>
After completion, create `.planning/phases/01-backend-security/01-02-SUMMARY.md`
</output>
