---
phase: 01-backend-security
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/types/api.ts
  - src/lib/openai.ts
  - src/lib/ratelimit.ts
  - src/lib/sanitize.ts
  - .env.example
  - vercel.json
autonomous: true

must_haves:
  truths:
    - "OpenAI client is configured with 10-second timeout and zero retries"
    - "Rate limiter enforces both hourly and daily limits per IP using sliding window"
    - "Prompt injection patterns are detected and blocked before reaching LLM"
    - "Input over 200 characters is rejected"
    - "Environment variables are validated at module initialization"
  artifacts:
    - path: "src/types/api.ts"
      provides: "Shared API request/response types"
      exports: ["AskRequest", "AskResponse", "AskErrorResponse"]
    - path: "src/lib/openai.ts"
      provides: "Configured OpenAI client"
      exports: ["openai"]
    - path: "src/lib/ratelimit.ts"
      provides: "Hourly and daily rate limiters"
      exports: ["rateLimiter", "dailyRateLimiter"]
    - path: "src/lib/sanitize.ts"
      provides: "Input validation and prompt injection detection"
      exports: ["sanitizeInput"]
    - path: ".env.example"
      provides: "Environment variable template"
      contains: "OPENAI_API_KEY"
    - path: "vercel.json"
      provides: "Serverless function configuration"
      contains: "maxDuration"
  key_links:
    - from: "src/lib/ratelimit.ts"
      to: "@upstash/redis"
      via: "Redis constructor with REDIS_URL and REDIS_TOKEN"
      pattern: "new Redis"
    - from: "src/lib/openai.ts"
      to: "openai"
      via: "OpenAI constructor with OPENAI_API_KEY"
      pattern: "new OpenAI"
---

<objective>
Install backend dependencies, create shared API types, configure environment variable template and Vercel function settings, and build the three core library modules: OpenAI client, rate limiter, and input sanitizer.

Purpose: Establish the foundational modules that the API endpoint (Plan 02) will compose into a working serverless function. Each module is independently testable and has a single responsibility.

Output: Installed `openai`, `@upstash/ratelimit`, `@upstash/redis` packages; shared types; configured OpenAI client; dual rate limiters; prompt injection detector; `.env.example` template; `vercel.json` with function settings.
</objective>

<execution_context>
@/Users/august/.claude/get-shit-done/workflows/execute-plan.md
@/Users/august/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-backend-security/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install dependencies and create project infrastructure</name>
  <files>package.json, src/types/api.ts, .env.example, vercel.json</files>
  <action>
1. Install production dependencies:
   ```
   npm install openai @upstash/ratelimit @upstash/redis
   ```

2. Create `src/types/api.ts` with shared API types:
   - `AskRequest`: `{ question: string }`
   - `AskResponse`: `{ lyric: string }`
   - `AskErrorResponse`: `{ error: string; retryAfter?: number }` (retryAfter is seconds until rate limit reset — Claude's discretion on exact structure per CONTEXT.md)
   - Export all types

3. Create `.env.example` with required environment variables:
   ```
   # OpenAI API Key (get from https://platform.openai.com/api-keys)
   OPENAI_API_KEY=sk-...

   # Redis credentials (auto-injected by Vercel Marketplace Upstash integration in production)
   # For local development, create a free Upstash Redis instance at https://console.upstash.com
   REDIS_URL=https://...
   REDIS_TOKEN=...
   ```

4. Create `vercel.json` with function configuration:
   ```json
   {
     "functions": {
       "api/ask.ts": {
         "maxDuration": 15,
         "memory": 1024
       }
     }
   }
   ```
   maxDuration set to 15 seconds — slightly above the 10-second OpenAI timeout to allow for overhead while still failing fast.

5. Add `.env.local` to `.gitignore` if not already present.
  </action>
  <verify>
- `npm ls openai @upstash/ratelimit @upstash/redis` shows all three packages installed
- `src/types/api.ts` exports AskRequest, AskResponse, AskErrorResponse types
- `.env.example` exists with OPENAI_API_KEY, REDIS_URL, REDIS_TOKEN placeholders
- `vercel.json` exists with maxDuration: 15 for api/ask.ts
- `npm run build` still succeeds (no type errors introduced)
  </verify>
  <done>All backend dependencies installed, shared API types defined, environment variable template created, Vercel function configured with 15-second max duration.</done>
</task>

<task type="auto">
  <name>Task 2: Create OpenAI client and rate limiter modules</name>
  <files>src/lib/openai.ts, src/lib/ratelimit.ts</files>
  <action>
1. Create `src/lib/openai.ts`:
   - Import `OpenAI` from `openai` package
   - Validate `OPENAI_API_KEY` environment variable exists at module initialization — throw descriptive error if missing
   - Create and export `openai` client instance with:
     - `apiKey`: from `process.env.OPENAI_API_KEY`
     - `timeout`: `10 * 1000` (10 seconds — locked decision from CONTEXT.md)
     - `maxRetries`: `0` (disable retries for predictable behavior in serverless — per research recommendation)

2. Create `src/lib/ratelimit.ts`:
   - Import `Ratelimit` from `@upstash/ratelimit` and `Redis` from `@upstash/redis`
   - Create Redis client with `process.env.REDIS_URL` and `process.env.REDIS_TOKEN`
   - Validate both env vars exist at initialization — throw descriptive error if missing
   - Create and export `rateLimiter` with sliding window: **5 requests per 1 hour** (research suggested 5/hour — reasonable: allows 1 question + 4 retries/variations per hour)
   - Create and export `dailyRateLimiter` with sliding window: **75 requests per 24 hours** (Claude's discretion: 75 is the midpoint of the locked 50-100 range — generous enough for real users, tight enough to block automation)
   - Use distinct prefixes: `"wwts:hourly"` for hourly, `"wwts:daily"` for daily

Note: These modules use `process.env` which is only available in the serverless runtime, not in the browser. They will be imported only by `api/ask.ts` and never bundled into the frontend.
  </action>
  <verify>
- `src/lib/openai.ts` exports `openai` instance with timeout: 10000 and maxRetries: 0
- `src/lib/ratelimit.ts` exports `rateLimiter` (5/hour) and `dailyRateLimiter` (75/day)
- Both modules validate their required environment variables at initialization
- TypeScript compiles without errors: `npx tsc --noEmit --project tsconfig.app.json` (may need to skip these files if tsconfig.app.json only includes `src/` — verify tsconfig includes src/lib/)
  </verify>
  <done>OpenAI client configured with 10-second timeout and zero retries. Dual rate limiters configured with sliding window algorithm — 5/hour and 75/day — using Upstash Redis for persistent state.</done>
</task>

<task type="auto">
  <name>Task 3: Create input sanitizer with prompt injection detection</name>
  <files>src/lib/sanitize.ts</files>
  <action>
Create `src/lib/sanitize.ts` with the following:

1. Define `INJECTION_PATTERNS` array of RegExp patterns to detect common prompt injection attempts:
   - "ignore (all) previous/prior/above instructions" variants
   - "disregard previous/prior/system prompt/instructions" variants
   - "you are now (in) developer/admin/debug mode" variants
   - "bypass safety/security/content checks/filters" variants
   - "reveal hidden/system/internal prompt/data/instructions" variants
   - "roleplay as system/admin/developer" variants
   - "output your (system) prompt/instructions" variants
   - Fuzzy matching variants for common misspellings/obfuscation (e.g., "ign0re", "bypas+")
   - "act as" + privileged role patterns
   - "new instructions:" / "system:" prefix injection attempts

2. Export `sanitizeInput(input: string)` function that returns `{ safe: boolean; error?: string }`:
   - If `input.length > 200`: return `{ safe: false, error: "I couldn't understand that one. Try asking differently?" }` (locked decision: 200 char limit)
   - If `input.trim().length === 0`: return `{ safe: false, error: "I couldn't understand that one. Try asking differently?" }` (empty input is invalid but same warm message)
   - Normalize input for detection: `input.normalize('NFKC').toLowerCase().replace(/\s+/g, ' ').trim()` (Unicode normalization per research pitfall #5)
   - Test normalized input against all INJECTION_PATTERNS
   - If any pattern matches: return `{ safe: false, error: "I couldn't understand that one. Try asking differently?" }` (locked decision: same vague warm message for all rejections — don't reveal detection logic)
   - If no issues: return `{ safe: true }`

3. Important: No minimum input length — single words like "love" and "why" are valid (locked decision).

4. The error message for ALL rejection cases is identical: "I couldn't understand that one. Try asking differently?" — this is intentional per user decision to never reveal what triggered the block.
  </action>
  <verify>
- `src/lib/sanitize.ts` exports `sanitizeInput` function
- Function returns `{ safe: true }` for normal questions like "Why does love hurt?"
- Function returns `{ safe: false, error: "..." }` for inputs over 200 characters
- Function returns `{ safe: false, error: "..." }` for empty/whitespace input
- Function returns `{ safe: false, error: "..." }` for injection attempts like "ignore previous instructions"
- Function returns `{ safe: true }` for single words like "love", "why", "heartbreak"
- All error messages are identical: "I couldn't understand that one. Try asking differently?"
  </verify>
  <done>Input sanitizer detects prompt injection via regex patterns with Unicode normalization, enforces 200-character limit, allows single-word inputs, and returns identical warm error message for all rejection types.</done>
</task>

</tasks>

<verification>
1. All three packages installed: `npm ls openai @upstash/ratelimit @upstash/redis`
2. Type check passes: `npx tsc --noEmit` (may need adjustment for src/lib modules that use process.env — these are server-only)
3. All exports present: openai client, rateLimiter, dailyRateLimiter, sanitizeInput, API types
4. No secrets in committed code — only .env.example with placeholder values
5. .env.local is gitignored
</verification>

<success_criteria>
- Three library modules (openai.ts, ratelimit.ts, sanitize.ts) exist with correct exports
- Shared API types defined for request/response shapes
- Environment variable template documents all required secrets
- Vercel function configured with 15-second max duration
- All dependencies installed and importable
</success_criteria>

<output>
After completion, create `.planning/phases/01-backend-security/01-01-SUMMARY.md`
</output>
