---
phase: 02-core-matching
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - api/ask.ts
autonomous: false

must_haves:
  truths:
    - "Production deployment at taylor-opal.vercel.app returns real matched lyrics"
    - "Emotional questions return contextually fitting Taylor Swift lyrics"
    - "Questions with no good match return soft poetic fallback messages"
  artifacts:
    - path: "api/ask.ts"
      provides: "Deployed and verified matching pipeline"
  key_links:
    - from: "api/ask.ts (deployed)"
      to: "Supabase pgvector"
      via: "SUPABASE_URL + SUPABASE_ANON_KEY env vars in Vercel"
      pattern: "SUPABASE_URL"
    - from: "api/ask.ts (deployed)"
      to: "OpenAI embeddings + chat"
      via: "OPENAI_API_KEY env var in Vercel"
      pattern: "OPENAI_API_KEY"
---

<objective>
Deploy the matching engine to Vercel and verify end-to-end lyric matching works in production.

Purpose: The matching pipeline is implemented locally, but needs production verification. This plan deploys to Vercel (Supabase env vars were added in 02-01), runs real test queries, and validates that emotional matching works with actual data.

Output: Verified production deployment with working lyric matching.
</objective>

<execution_context>
@/Users/august/.claude/get-shit-done/workflows/execute-plan.md
@/Users/august/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-matching/02-01-SUMMARY.md
@.planning/phases/02-core-matching/02-02-SUMMARY.md
@api/ask.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Deploy to Vercel and run automated verification</name>
  <files>api/ask.ts</files>
  <action>
**Step 1: Verify Supabase env vars exist in Vercel**

```bash
vercel env ls
```

Confirm SUPABASE_URL and SUPABASE_ANON_KEY are present (added by user in 02-01 checkpoint). If missing, abort and inform user.

**Step 2: Deploy to production**

```bash
vercel --prod
```

Wait for deployment to complete. Note the production URL (should be https://taylor-opal.vercel.app).

**Step 3: Run automated test queries**

Test the full pipeline with curl. Each test validates a different aspect:

```bash
# Test 1: Emotional question (should return a real lyric)
curl -s -X POST https://taylor-opal.vercel.app/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "How do I get over a breakup?"}'

# Test 2: Different emotional tone (should return a different lyric)
curl -s -X POST https://taylor-opal.vercel.app/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "I feel like nobody understands me"}'

# Test 3: Empowerment question
curl -s -X POST https://taylor-opal.vercel.app/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "How do I believe in myself again?"}'

# Test 4: Off-topic question (may trigger fallback)
curl -s -X POST https://taylor-opal.vercel.app/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the meaning of quantum physics?"}'
```

**Step 4: Validate responses**

For each response, check:
- Response is valid JSON with `lyric` field
- Lyric is 1-2 lines max (not a full verse or song)
- Lyric is ONLY the lyric text (no explanations, no "Here's a lyric for you:")
- Emotional questions return a Taylor Swift lyric (not a fallback)
- Off-topic questions may return either a loosely relevant lyric or a poetic fallback — both are acceptable

**Step 5: Test error handling preserved**

```bash
# Empty question (should return 400)
curl -s -X POST https://taylor-opal.vercel.app/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": ""}'

# Missing question field (should return 400)
curl -s -X POST https://taylor-opal.vercel.app/api/ask \
  -H "Content-Type: application/json" \
  -d '{}'

# GET method (should return 405)
curl -s https://taylor-opal.vercel.app/api/ask
```

If any test fails with 500 errors, check Vercel function logs for debugging:
```bash
vercel logs --output raw 2>&1 | tail -50
```

Common issues:
- 500 with "SUPABASE_URL" error: env vars not set, re-add them
- 500 with embedding error: OPENAI_API_KEY issue
- Empty lyric responses: similarity threshold too strict (lower from 0.70 to 0.65)
- All responses identical: temperature too low or candidates always same 3

If similarity threshold needs adjustment, update the `match_threshold` value in api/ask.ts and redeploy.
  </action>
  <verify>
- `vercel --prod` deploys successfully
- Test queries 1-3 return valid `{ lyric: "..." }` responses with real Taylor Swift lyrics
- Lyrics are 1-2 lines, lyric text only (no commentary)
- Error handling tests return correct status codes (400, 405)
- No 500 errors on valid questions
  </verify>
  <done>
Production deployment returns real matched lyrics for emotional questions. Error handling from Phase 1 is preserved. The matching pipeline works end-to-end: question -> embedding -> pgvector -> LLM selection -> lyric response.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: User verifies matching quality</name>
  <files>api/ask.ts</files>
  <action>
What was built: Complete lyric matching engine deployed to production.

Test the matching quality by sending a few personal questions:

1. **Visit or curl** https://taylor-opal.vercel.app/api/ask with POST and a JSON body `{"question": "your question here"}`

2. **Try these emotional scenarios:**
   - A heartbreak question: "Why can't I stop thinking about them?"
   - A self-doubt question: "Am I good enough?"
   - An empowerment question: "How do I start over?"
   - A growth question: "When does it stop hurting?"

3. **Check each response:**
   - Does the lyric feel emotionally relevant to the question?
   - Is it just the lyric (no explanations or extras)?
   - Does it make you feel something? (The core value: "makes them feel understood")

4. **Try the same question twice** — you should get a different lyric at least some of the time (temperature > 0)

5. **Try an off-topic question** — you should get a poetic fallback message or a loosely relevant lyric

If the matching feels off:
- "Too literal" → Prompt needs tuning (note this for gap closure)
- "Too random" → Threshold may be too low (note specific examples)
- "Always same lyric" → Temperature or candidate diversity issue

Resume signal: Type "approved" if matching quality feels good, or describe specific issues for gap closure.
  </action>
  <verify>User confirms emotional matching quality meets the core value: "the lyric makes them feel understood."</verify>
  <done>Matching engine produces emotionally relevant lyrics for life questions, with variation on repeat queries and graceful fallbacks for poor matches.</done>
</task>

</tasks>

<verification>
- Production deployment returns lyrics for emotional questions
- Responses are lyric-only, 1-2 lines max
- Different emotional tones receive different lyrics
- Fallback messages appear for poor matches (soft and poetic, not error-like)
- Same question can return different lyrics on repeat
- Phase 1 error handling preserved (rate limiting, sanitization, 400/405/429 codes)
</verification>

<success_criteria>
The lyric matching engine is live in production: emotional questions return contextually fitting Taylor Swift lyrics, the matching prioritizes emotional vibe over keywords, and the overall experience delivers on the core value — "the lyric makes them feel understood."
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-matching/02-03-SUMMARY.md`
</output>
